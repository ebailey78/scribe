# LLM Selection & Deployment

**Priority:** HIGH - Final AI Engine

## Model Requirements

| Requirement | Justification based on Pipeline Stage | Source Reference |
| :--- | :--- | :--- |
| **Long-context stability** | Essential for synthesizing the multi-chunk input generated by the **Map-Reduce workflow (L5)**. This necessitates models that excel at scaling to extremely long texts (up to 128K+ tokens). | |
| **Instruction adherence** | Critical for consistently executing the complex, iterative prompt structure of the **Chain of Density (CoD) technique (L6)**. | |
| **Structured output** | Mandatory for the **CoD phase (L6)**, which requires the output to be validated in a reliable format, typically **JSON**. | |
| **VRAM fit** | The total VRAM required (quantized weights + KV Cache + overhead) must be **$\le$ 6GB**. Q4\_K\_M model weights should ideally be $\le$ 5GB to allow for necessary overhead. | |

## Recommended Models

The selection prioritizes long-context stability, as this is the most critical requirement for the Map-Reduce architecture.

| Model | Size (Q4\_K\_M) | Strengths | Use Case |
| :--- | :--- | :--- | :--- |
| **LLaMA 3.1 8B Instruct** | $\approx$ 4.9 GB | **Superior long-context scaling capability** (up to 128K+ tokens); strong general performance; effective JSON output. | Final Synthesis (Reduce/CoD) |
| **Qwen 2.5 7B Chat** | $\approx$ 4.7 GB | **Optimal VRAM fit**; high synthesis quality; **exceptional structured output/JSON adherence**. | Alternative for Synthesis |
| **Mistral 7B** | $\approx$ 4.37 GB | Highly optimized for low VRAM (requiring $\approx$ 6.87 GB memory); excellent for fast, self-contained AI inference. | Secondary Alternative / Map Phase Offload |

**Primary:** The **LLaMA 3.1-8B-Instruct** model is the **strategically optimal model choice** for the core summarization task, given the data indicating its superior long-context scaling capability.

## Quantization

**Format:** **GGUF Q4\_K\_M** (mandatory)

**Why:**
*   **Quantization** is the cornerstone technology that makes running large language models feasible on consumer-grade hardware. It reduces the memory footprint of a 7B model from $\approx 14$ GB (FP16) to $\approx 4-5$ GB using 4-bit quantization.
*   The **Q4\_K\_M format** is the mandated format because it offers the best balance of file size, quality, and inference speed for constrained VRAM environments.

**Avoid:** **Q2\_K** (noticeable quality loss). While Q2\_K offers further compression, it may lead to a noticeable degradation in model quality.

## Deployment

**Engine:** **`llama-cpp-python`**. This engine, built on high-performance C++ inference (`llama.cpp`), is required for running GGUF models and offers efficient inference on consumer hardware.

```bash
pip install llama-cpp-python
```

**Configuration:** The configuration must allocate memory to the GPU carefully.

```python
from llama_cpp import Llama

llm = Llama(
    model_path="models/llama-3.1-8b-instruct.Q4_K_M.gguf",
    n_ctx=8192,           # Context window
    n_gpu_layers=-1,      # Offload all layers to GPU
    verbose=False,
    # Additional optimization flags are recommended below.
)
```

**GPU Layer Control (`-ngl`):**
*   `llama.cpp` supports **CPU+GPU hybrid inference** to partially accelerate models larger than the total VRAM capacity.
*   The `-ngl` (Number of GPU Layers) flag controls how many layers are offloaded to the GPU.
*   `-1`: Offloads all layers to GPU (if VRAM permits).
*   `0`: CPU-only fallback.
*   `N`: Hybrid execution where N layers are placed on the GPU and the remaining layers (N+1 to end) are placed on the CPU/System RAM.

## VRAM Optimization

Optimization techniques are mandatory to ensure the model fits and supports long context lengths within the strict 6GB VRAM budget.

**Flash Attention:** **Flash Attention** is a memory-efficient attention mechanism that scales linearly with sequence length. It provides a **2x-4x speedup** on multi-image and video scenarios and reduces memory consumption by **20-30%**.

**KV Cache Quantization:** The **Key-Value (KV) Cache** for context history alone can consume over 5GB of VRAM in FP16 for 10,000 tokens. The pipeline must utilize **KV Cache Quantization**.

```python
llm = Llama(
    ...,
    cache_type_k="q8_0",  # 8-bit cache (2x memory savings)
    cache_type_v="q8_0"
)
```

**Impact:** **2x-4x reduction** in cache footprint $\rightarrow$ enables support for **significantly longer meetings**.

## Integration Points

The asynchronous workflow leverages heterogeneous hardware for different computational loads:

1.  **Map Phase:** The task of generating distilled mini-summaries from small chunks (e.g., **800 tokens**) requires minimal deep reasoning. This process is ideally offloaded to the **Intel NPU** via **OpenVINO**. The NPU's static prompt limitation (typically 1024 tokens) means it is restricted to processing these small Map segments.
2.  **Reduce Phase:** The **consolidated input** (summary of summaries) is handled by the primary, more powerful **GPU LLM**. The GPU is reserved for this final synthesis step to ensure high-quality, nuanced output.
3.  **CoD Phase:** The final iterative refinement is executed by the **GPU LLM**, leveraging its superior instruction adherence for the complex prompt structure.

## Alternatives

**Ollama:** Ollama is an optimized tool for running local LLMs and supports the GGUF format. Ollama natively supports memory management using a "Keep-Alive" feature, which can unload models after a timeout, freeing up $\approx 6$ GB of VRAM. However, **`llama-cpp-python`** offers **more fine-grained control** over VRAM allocation, such as manually setting the number of GPU layers (`-ngl`), which is crucial for maximizing efficiency and stability in the tight **6GB VRAM constraint**.

## Validation

Validation is required to ensure the context management and LLM scaling strategies are successful.

Test context scaling:
```python
# Verify model handles Map-Reduce output size
test_input = "..." * 5000  # ~10k tokens
output = llm(test_input, max_tokens=500)
```
The pipeline must verify that the chosen model can reliably process input sizes exceeding 10,000 tokens, which is the expected size of a one-hour meeting transcript. Failure in validation may indicate that the Map-Reduce workflow has not sufficiently compressed the input or that the KV Cache quantization is inadequate.