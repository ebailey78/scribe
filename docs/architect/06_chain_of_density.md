# Chain of Density (L6)

**Priority:** HIGH - Output Refinement

## Objective

The primary objective of CoD prompting is to **maximize information density** and **minimize verbosity** ("fluff") in the final summary. CoD addresses the tendency of general-purpose LLMs to produce summaries that are **verbose and entity-sparse**. By forcing denser output, the system extracts the maximum information value per token generated, mitigating the inherent performance gap between a small local model and larger, state-of-the-art closed models.

## When to Apply

The Chain of Density technique must be **strategically integrated only at the very end of the pipeline**.

*   **Execution Point:** It is applied **exclusively to the final, consolidated output** generated by the **Reduce step** of the Map-Reduce workflow (L5).
*   **Rationale:** Applying CoD to every small segment during the Map stage would lead to **massive computational overhead** and unnecessary redundancy. Reserving CoD for the final synthesis ensures the small LLM’s limited processing cycles are optimally directed toward maximizing the informational density of the final output.

## Iterative Process

CoD transforms summarization into a sophisticated, iterative refinement procedure. The process involves generating a sequence of summaries that become progressively denser while strictly maintaining the original length.

```
Loop 3-5 times:
  1. Identify 1-3 novel, salient entities (missing from prior iteration)
  2. Rewrite summary at SAME LENGTH, integrating new entities
```

*   **Initial Step:** The model first generates an initial **entity-sparse** summary, containing few key nouns or noun phrases. This establishes the fixed token count.
*   **Iterative Rewriting:** The model must rewrite the previous summary entirely in each subsequent step, specifically to **include one or two new, key entities** extracted from the source material. This is typically repeated for **five fixed iterations** according to the standard template.

**Research Note:** **Human preference peaks at Step 3**. Research comparing CoD summaries to human-written summaries found that the **median preferred CoD step was step 3**, indicating a balance in summary density. Summaries generated around Step 3 most closely mimic the **density of human-written summaries**, ideally around **0.15 entities per token**.

## Entity Criteria (ALL required)

The entities identified for integration must adhere to a strict set of rules:

*   **Relevant:** Must pertain to the main narrative or story.
*   **Specific:** Must be descriptive yet **concise (five words or fewer)**.
*   **Novel:** Must **not** have been present in the previous summary iteration.
*   **Faithful:** Must be **verifiably present** in the source text (the original article/transcript).
*   **Anywhere:** The entity can be located anywhere in the source text.

## Constraints

The strict constraints are the core mechanics that enforce density (compression) over verbosity.

1.  **Never drop** entities from previous iterations. If space cannot be made for new entities, the model should add fewer new entities.
2.  **Fixed length** (exact word count maintained). The new summary must be of **identical length** to the previous iteration.
3.  **Compression:** To create space for new entities, the model must compress or rephrase existing content, using techniques such as **fusion** and the **removal of uninformative phrases** (e.g., "the article discusses"). This ensures the content becomes progressively more abstract and synthesized.

## Prompt Template

The required prompt template guides the LLM through the process, specifying the output format and constraints.

```markdown
Article: [Reduce output]

Repeat 2 steps, 5 times:
Step 1: Identify 1-3 Missing Entities (";"-delimited) not in previous summary
Step 2: Write new summary (SAME LENGTH) covering all prior entities + Missing

Entity Rules: Relevant, Specific (≤5 words), Novel, Faithful

Constraints:
- Never drop prior entities
- Exact same word count
- Output JSON: [{"Missing_Entities": "", "Denser_Summary": ""}]
```

## Implementation

*   **LLM Requirement:** The LLM must possess **strong instruction adherence** to execute the highly prescriptive iterative prompt. It must also be proficient in **structured output generation** (JSON).
*   **Recommended:** The **Qwen3-8B** model is cited as a **frontrunner** due to its **extensive long-context support** and **demonstrated strong instruction-following capabilities**, making it highly suited for the complexities of iterative CoD prompts. The LLaMA 3.1-8B-Instruct model is also effective for generating structured output, such as extracting concepts into a JSON payload.
*   **Output:** The final output must be in a structured **JSON format**.
*   **Format:** The JSON output must be a list (length 5) of dictionaries, where each dictionary contains the keys "Missing\_Entities" and "Denser\_Summary".

## Why L3 Correction Matters

CoD relies heavily on the fidelity of the entities it integrates.

*   **Entity Fidelity:** The success of the entity-focused refinement is directly supported by the preceding Stage 3/L3 **Jargon Correction**.
*   **Guaranteed Accuracy:** The L3 stage utilizes **fuzzy matching** (Levenshtein distance) to correct ASR misrecognitions of domain-specific jargon. This step ensures that all key entities are **100% accurate and faithful to the canonical list** *before* the LLM sees them, which directly boosts the effectiveness of the entity-focused Chain of Density technique.

## Performance

*   **Iterations:** The standard template repeats the process **5 times**. However, human evaluations suggest sticking to **2 or 3 steps** for summaries that align with human preferences for density.
*   **Trade-off:** The CoD process involves **multiple processing steps**, which introduces **computational overhead** compared to single-pass summarization, requiring a trade-off between quality (higher iterations) and latency.
*   **Format:** The final format must be **JSON**.