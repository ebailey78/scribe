# Chain of Density (CoD) Prompting (Output Refinement)

**Category:** LLM Workflows and Optimization  
**Stage:** Stage 5

## Overview

The Chain of Density (CoD) prompting technique is the final architectural step in the optimized pipeline, deployed during the synthesis phase (Stage 5/6). This stage is critical because it addresses the tendency of Large Language Models (LLMs) to produce summaries that are **verbose and entity-sparse** ("fluff"). CoD transforms abstractive summarization into a meticulous, iterative refinement procedure focused on maximizing the **information density** of the final output.

By forcing the LLM to integrate more specific entities while strictly maintaining a fixed output length, CoD minimizes the performance gap between small local models (like 7B–14B models) and larger, state-of-the-art closed models.

## Iterative Refinement Process

The CoD process involves generating a sequence of summaries that become progressively denser through forced iterative rewriting. This workflow is applied exclusively to the **final, consolidated output** generated by the Map-Reduce workflow (the Reduce step).

The typical steps mandated by the prompt structure are:

1.  **Initial Summary:** The LLM first generates a baseline summary that is intentionally non-specific and entity-sparse, establishing the fixed length constraint.
2.  **Entity Identification:** The model is instructed to identify unique, high-value **missing entities** from the original source material that are absent from the preceding summary iteration.
3.  **Rewriting and Integration:** The model then writes a **new summary of identical length**, incorporating the newly identified missing entities.
4.  **Repetition:** This two-step process (Identify and Rewrite) is repeated for a fixed number of iterations, typically **5 times** in the prompt structure. However, practical implementation often suggests fewer steps, as the median preferred CoD step chosen by human evaluators was **Step 3**.

## Novel Entity Integration

The success of the Chain of Density technique relies on the strict quality control placed on the entities identified for integration.

### 1-3 Entities Per Iteration

In each subsequent iteration after the initial summary, the model must identify and integrate **1 to 3 new salient entities** into the existing summary.

A salient entity, often a key noun or noun phrase, must adhere to the following precise criteria:

*   **Relevant:** Must pertain to the main narrative or story.
*   **Specific:** Must be descriptive yet **concise** (specifically **five words or fewer**).
*   **Novel:** Must **not** have been present in any previous summary iteration.
*   **Faithful:** Must be **verifiably present** in the source text.
*   **Anywhere:** Must be located anywhere in the original source text.

The fidelity of these key entities is paramount, which is why prior architectural stages, such as L3 Domain Correction (Jargon Fidelity) via fuzzy matching, are considered **architectural necessities** to ensure terms are 100% faithful before the LLM sees them.

## Length Maintenance Through Compression

The defining characteristic of CoD is that it forces the LLM to generate progressively richer summaries without increasing the output length.

*   **Fixed Length Constraint:** The model must strictly maintain the same fixed length (e.g., the exact number of words) for each summary iteration.
*   **Compression Mechanics:** To accommodate the 1–3 new entities in the limited space, the LLM must utilize efficient compression methods:
    *   **Fusion and Abstraction:** Content must be fused, reorganized, and abstracted.
    *   **Removal of Uninformative Phrases:** The model must remove verbose or uninformative phrases (e.g., "the article discusses").
*   **Non-Negotiable Retention:** A critical constraint is that the model must **never drop entities** that were present in any previous summary iteration. If necessary, the model must add fewer new entities to maintain this constraint.

## Required Prompt Structure

The template below guides the LLM through the necessary steps and constraints for the CoD process, typically repeated **five times**.

```markdown
Article: [SOURCE TEXT (Consolidated Output of Map-Reduce Step)]

You will generate increasingly concise, entity-dense summaries of the above Article. Repeat the following 2 steps 5 times.

Step 1. Identify 1-3 informative Entities (";" delimited) from the Article which are missing from the previously generated summary.

Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. The Missing Entities must be highlighted in bold text.

A Missing Entity is:
- Relevant: to the main story.
- Specific: descriptive yet concise (5 words or fewer).
- Novel: not in the previous summary.
- Faithful: present in the Article.

Constraint Checklist:
1. Never drop entities from the previous summary. If space cannot be made, add fewer new entities.
2. Remember, use the exact same number of words for each summary.
3. The final summaries should be highly dense and concise, yet self-contained (easily understood without the Article).

Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are "Missing_Entities" and "Denser_Summary".
```

## Implementation Guidelines

1.  **Placement:** CoD is performed in **Stage 5** (Output Refinement) on the final, condensed output of the **Reduce step** of the Map-Reduce workflow.
2.  **LLM Selection:** The chosen LLM must excel at **instruction adherence** (for executing the complex iterative prompt) and **structured output** (JSON generation). Candidates like **Qwen 2.5 14B** are noted for exceptional adherence to JSON schemas.
3.  **Efficiency:** It must not be applied to every segment in the Map phase, as this leads to massive computational overhead.
4.  **Hardware:** Due to the iterative nature, CoD processes multiple steps, contributing to **computational overhead** compared to single-pass summarization. This makes the choice of an optimized, quantized LLM (e.g., GGUF format) critical for local deployment.
5.  **Iteration Count:** While the prompt template specifies 5 iterations, practical research suggests that **2 or 3 steps** produce summaries most in line with human-preferred density (around 0.15 entities per token).

## Examples

The iterative process transforms a broad statement into a dense, abstract summary over several steps:

| Iteration | Key Action | Example Summary Evolution |
| :--- | :--- | :--- |
| **Step 1 (Initial)** | Generates baseline summary (sparse). | A race car was involved in an accident, leading to a delay and subsequent investigations. |
| **Step 2** | Adds 1-3 novel entities (e.g., Driver Name, Team). | **Max Verstappen**’s race car was involved in an accident, leading to a delay for the **Red Bull** team and subsequent investigations. |
| **Step 3 (Median Preference)** | Adds 1-3 novel entities (e.g., Specific Penalty, Location). | After the incident at **Turn 4**, Max Verstappen’s Red Bull car incurred a **10-second penalty**, resulting in a delay and further investigation. |
| **Step 4/5 (Refined)** | Integrates technical specifics (e.g., Lap Number, Damage). | After the incident at Turn 4, Max Verstappen’s damaged Red Bull car incurred a 10-second penalty **on lap 14**, delaying the team's ability to maintain its **P3 position**. |
